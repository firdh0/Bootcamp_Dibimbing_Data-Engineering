{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f717ed52",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 0: Importing Required Libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "727f2cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import logging\n",
    "import yaml\n",
    "import os \n",
    "\n",
    "from datetime import date\n",
    "from sqlalchemy import create_engine\n",
    "from logging.handlers import RotatingFileHandler\n",
    "from typing import Dict, Optional, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c006d2f3",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 1: Setting Up a Custom Logger for ETL Pipeline**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "14132850",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"\n",
    "    A logging utility class for managing and configuring loggers for the ETL pipeline.\n",
    "\n",
    "    This class sets up a logger that writes logs to both a rotating file and the console.\n",
    "    Logs are saved in a 'logs' directory located one level above the current file.\n",
    "    It uses Python's built-in `logging` module with `RotatingFileHandler` to handle\n",
    "    log rotation, ensuring that log files do not grow indefinitely.\n",
    "\n",
    "    Methods:\n",
    "        __init__():\n",
    "            Initializes the Logger instance with a specified name and log level.\n",
    "\n",
    "        _setup_logging(): \n",
    "            Configures the logger with file and console handlers.\n",
    "            \n",
    "        get_logger() -> logging.Logger:\n",
    "            Returns the configured logger instance for use in other parts of the application.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str = __name__, log_level: int = logging.DEBUG) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Logger instance.\n",
    "\n",
    "        Parameters:\n",
    "            name (str): The name of the logger. Defaults to the module's name.\n",
    "            log_level (int): Logging level to use (e.g., logging.DEBUG). Defaults to DEBUG.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the log directory cannot be created or accessed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger = logging.getLogger(name)\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        try:\n",
    "            base_dir = os.path.dirname(os.path.dirname(__file__))\n",
    "        except NameError:\n",
    "            base_dir = os.getcwd()\n",
    "\n",
    "        self.log_dir = os.path.join(base_dir, 'logs')\n",
    "        self._setup_logging()\n",
    "\n",
    "    def _setup_logging(self) -> None:\n",
    "        \"\"\"\n",
    "        Internal method to set up file and console handlers with formatting and rotation.\n",
    "\n",
    "        - Creates the logs directory if it doesn't exist.\n",
    "        - Sets up a rotating file handler (5 MB max, up to 5 backups).\n",
    "        - Adds a console stream handler.\n",
    "        - Applies consistent formatting to both handlers.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "\n",
    "        self.logger.handlers.clear()\n",
    "\n",
    "        file_handler = RotatingFileHandler(\n",
    "            filename=f'{self.log_dir}/etl_pipeline.log',\n",
    "            maxBytes=1024 * 1024 * 5,  # 5 MB\n",
    "            backupCount=5\n",
    "        )\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        console_handler.setFormatter(formatter)\n",
    "\n",
    "        self.logger.addHandler(file_handler)\n",
    "        self.logger.addHandler(console_handler)\n",
    "\n",
    "        self.logger.propagate = False\n",
    "\n",
    "    def get_logger(self) -> logging.Logger:\n",
    "        \"\"\"\n",
    "        Get the configured logger instance.\n",
    "\n",
    "        Returns:\n",
    "            logging.Logger: The logger configured with file and console handlers.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9eb1c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     logger = Logger(__name__).get_logger()\n",
    "#     logger.debug(\"Ini adalah pesan debug.\")\n",
    "#     logger.info(\"Ini adalah pesan info.\")\n",
    "#     logger.warning(\"Ini adalah pesan peringatan.\")\n",
    "#     logger.error(\"Ini adalah pesan kesalahan.\")\n",
    "#     logger.critical(\"Ini adalah pesan kritikal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21e828",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 2: Loading Configs and Setting Up Database Access**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3cdbaae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseConnection:\n",
    "    \"\"\"\n",
    "    A class for managing PostgreSQL database connections using both psycopg2 and SQLAlchemy.\n",
    "\n",
    "    This class provides methods to load configuration from a YAML file, establish connections \n",
    "    with psycopg2 for raw SQL operations, and SQLAlchemy for ORM or advanced queries.\n",
    "    It also supports context manager protocol for automatic connection management.\n",
    "\n",
    "    Methods:\n",
    "        __init__(config_path: str) -> None:\n",
    "            Initializes the DatabaseConnection instance with a configuration path.\n",
    "            \n",
    "        load_db_config() -> dict:\n",
    "            Loads database configuration from a YAML file.\n",
    "\n",
    "        connect() -> tuple[psycopg2.extensions.connection, sqlalchemy.engine.base.Engine]:\n",
    "            Establishes and returns both psycopg2 and SQLAlchemy connections.\n",
    "\n",
    "        close() -> None:\n",
    "            Closes the psycopg2 connection.\n",
    "\n",
    "        __enter__() -> tuple[psycopg2.extensions.connection, sqlalchemy.engine.base.Engine]:\n",
    "            Enables usage with context managers (with statement).\n",
    "\n",
    "        __exit__(exc_type, exc_val, exc_tb) -> None:\n",
    "            Automatically closes the connection at the end of a with block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DatabaseConnection instance.\n",
    "\n",
    "        Parameters:\n",
    "            config_path (str): The path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.config_path = config_path\n",
    "        self.connection = None\n",
    "        self.engine = None\n",
    "        self.logger = Logger(__name__).get_logger()  # Inisialisasi logger\n",
    "\n",
    "    def load_db_config(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load the database configuration from the YAML file.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing database connection settings.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the configuration file cannot be read or parsed.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            with open(self.config_path, 'r') as file:\n",
    "                return yaml.safe_load(file)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load database configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    def connect(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Establish connections to the PostgreSQL database using psycopg2 and SQLAlchemy.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - psycopg2 connection object\n",
    "                - SQLAlchemy engine object\n",
    "\n",
    "        Raises:\n",
    "            Exception: If the connection attempt fails.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            config = self.load_db_config()\n",
    "            db_config = config['postgresql']\n",
    "\n",
    "            # Koneksi menggunakan psycopg2\n",
    "            self.connection = psycopg2.connect(\n",
    "                host=db_config['host'],\n",
    "                database=db_config['database'],\n",
    "                user=db_config['user'],\n",
    "                password=db_config['password']\n",
    "            )\n",
    "            self.logger.info(\"Database connection successfully opened.\")\n",
    "\n",
    "            conn_string = f\"postgresql+psycopg2://{db_config['user']}:{db_config['password']}@{db_config['host']}/{db_config['database']}\"\n",
    "            self.engine = create_engine(conn_string)\n",
    "            return self.connection, self.engine\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to connect to database: {e}\")\n",
    "            raise\n",
    "\n",
    "    def close(self) -> None:\n",
    "        \"\"\"\n",
    "        Close the psycopg2 database connection.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            Exception: If closing the connection fails.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.connection:\n",
    "            try:\n",
    "                self.connection.close()\n",
    "                self.logger.info(\"Database connection closed.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to close database connection: {e}\")\n",
    "                raise\n",
    "\n",
    "    def __enter__(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Support for context manager entry. Calls `connect()`.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The result of the `connect()` method.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.connect()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n",
    "        \"\"\"\n",
    "        Support for context manager exit. Calls `close()`.\n",
    "\n",
    "        Parameters:\n",
    "            exc_type: Exception type, if raised.\n",
    "            exc_val: Exception value, if raised.\n",
    "            exc_tb: Traceback, if raised.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eba0142b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     db = DatabaseConnection('./config.yaml')\n",
    "#     conn, engine = db.connect()\n",
    "#     db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e9017",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 3: Checking and Cleaning Data Quality**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9a731a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQuality:\n",
    "    \"\"\"\n",
    "    A class to perform various data quality checks and cleaning operations\n",
    "    on a collection of datasets.\n",
    "\n",
    "    Methods:\n",
    "        __init__(datasets: Dict[str, pd.DataFrame]) -> None:\n",
    "            Initialize the DataQuality class with datasets.\n",
    "            \n",
    "        check_duplicates() -> pd.DataFrame:\n",
    "            Check for duplicate rows in each dataset and return a DataFrame of duplicates.\n",
    "        \n",
    "        check_null_values() -> pd.DataFrame:\n",
    "            Check for null values in each dataset and return a summary DataFrame.\n",
    "\n",
    "        handle_duplicates(duplicates_df: pd.DataFrame) -> None:\n",
    "            Remove duplicate rows from datasets based on 'url' column.\n",
    "\n",
    "        impute_value(df: pd.DataFrame, column_name: str, method: str, dataset_name: str) -> None:\n",
    "            Impute missing values in a specified column using a selected method.\n",
    "\n",
    "        handle_null_values(null_values_df: pd.DataFrame, drop_null_threshold: float = 20.0) -> None:\n",
    "            Handle missing values by dropping or imputing based on threshold and data type.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datasets: Dict[str, pd.DataFrame]) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataQuality class with datasets.\n",
    "\n",
    "        Parameters:\n",
    "            datasets (Dict[str, pd.DataFrame]): Dictionary mapping dataset names to DataFrames.\n",
    "        \"\"\"\n",
    "\n",
    "        self.datasets = datasets\n",
    "        self.logger = Logger(__name__).get_logger()\n",
    "\n",
    "    def check_duplicates(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Check for duplicate rows in each dataset.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Combined DataFrame listing all duplicated rows across datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_duplicates = pd.DataFrame(columns=['Dataset'])\n",
    "\n",
    "        for name, df in self.datasets.items():\n",
    "            duplicate_count = df.duplicated(keep='first').sum()\n",
    "\n",
    "            if duplicate_count > 0:\n",
    "                self.logger.info(f\"Dataset '{name}' has {duplicate_count} duplicate rows.\")\n",
    "\n",
    "                duplicates_df = df[df.duplicated(keep='first')].copy()\n",
    "                duplicates_df['Dataset'] = name\n",
    "                duplicates_df['Jumlah Duplikasi'] = df.groupby(list(df.columns)).transform('size')\n",
    "\n",
    "                cols = ['Dataset'] + [col for col in duplicates_df.columns if col != 'Dataset']\n",
    "                duplicates_df = duplicates_df[cols]\n",
    "                \n",
    "                combined_duplicates = pd.concat([combined_duplicates, duplicates_df], ignore_index=True)\n",
    "            else:  \n",
    "                self.logger.info(f\"Dataset '{name}' has no duplicates.\")\n",
    "        return combined_duplicates\n",
    "\n",
    "    def check_null_values(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Check for null values in each dataset and summarize them.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Summary of null values per column and dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        combined_nulls = pd.DataFrame()\n",
    "        for name, df in self.datasets.items():\n",
    "            df = self._convert_to_datetime(df)\n",
    "            null_values = df.isnull().sum()\n",
    "            null_columns = null_values[null_values > 0]\n",
    "\n",
    "            if not null_columns.empty:\n",
    "                self.logger.info(f\"Dataset '{name}' has null values.\")\n",
    "                column_types = df.dtypes[null_columns.index]\n",
    "                is_date_type = column_types.apply(pd.api.types.is_datetime64_any_dtype)\n",
    "                null_value_summary = pd.DataFrame({\n",
    "                    'Dataset': name,\n",
    "                    'Kolom': null_columns.index,\n",
    "                    'Tipe Data': column_types.values,\n",
    "                    'Jumlah Null Values': null_columns.values,\n",
    "                    'Persentase Null Values (%)': (null_columns.values / len(df)) * 100,\n",
    "                    'Apakah Tipe Data Date?': is_date_type.values\n",
    "                })\n",
    "                combined_nulls = pd.concat([combined_nulls, null_value_summary], ignore_index=True)\n",
    "            else:\n",
    "                self.logger.info(f\"Dataset '{name}' has no null values.\")\n",
    "        return combined_nulls\n",
    "\n",
    "    def _convert_to_datetime(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert columns with 'date' or 'timestamp' in name to datetime.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with converted datetime columns.\n",
    "        \"\"\"\n",
    "\n",
    "        date_columns = [col for col in df.columns if 'date' in col.lower() or 'timestamp' in col.lower()]\n",
    "        for col in date_columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        return df\n",
    "\n",
    "    def handle_duplicates(self, duplicates_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Remove duplicate rows from datasets based on 'url' column.\n",
    "\n",
    "        Parameters:\n",
    "            duplicates_df (pd.DataFrame): DataFrame containing duplicate information.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        if 'Dataset' not in duplicates_df.columns:\n",
    "            self.logger.info(\"There are no duplicates to deal with.\")\n",
    "            return\n",
    "        \n",
    "        datasets_with_duplicates = duplicates_df['Dataset'].unique()\n",
    "        for dataset_name in datasets_with_duplicates:\n",
    "            if dataset_name in self.datasets:\n",
    "                self.logger.info(f\"Removing duplicates from dataset '{dataset_name}'.\")\n",
    "                self.datasets[dataset_name] = self.datasets[dataset_name].drop_duplicates(subset='url', keep='first')\n",
    "\n",
    "    def impute_value(self, df: pd.DataFrame, column_name: str, method: str, dataset_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Impute missing values in a specified column using a selected method.\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame to process.\n",
    "            column_name (str): Column to impute.\n",
    "            method (str): Imputation method ('mean', 'median', 'mode', 'interpolate').\n",
    "            dataset_name (str): Name of the dataset for logging.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the method is not recognized.\n",
    "        \"\"\"\n",
    "\n",
    "        if method == 'mean':\n",
    "            df[column_name] = df[column_name].fillna(df[column_name].mean())\n",
    "        elif method == 'median':\n",
    "            df[column_name] = df[column_name].fillna(df[column_name].median())\n",
    "        elif method == 'mode':\n",
    "            df[column_name] = df[column_name].fillna(df[column_name].mode()[0])\n",
    "        elif method == 'interpolate':\n",
    "            df[column_name] = df[column_name].interpolate()\n",
    "            df[column_name] = df[column_name].dt.floor('s')\n",
    "        else:\n",
    "            self.logger.error(f\"The imputation method '{method}' is not recognized.\")\n",
    "            raise ValueError(f\"Imputation method '{method}' is not recognized.\")\n",
    "        self.logger.info(f\"Imputation is performed on column '{column_name}' of dataset '{dataset_name}' using method '{method}'.\")\n",
    "\n",
    "    def handle_null_values(self, null_values_df: pd.DataFrame, drop_null_threshold: float = 20.0) -> None:\n",
    "        \"\"\"\n",
    "        Handle missing values by dropping or imputing based on threshold and data type.\n",
    "\n",
    "        Parameters:\n",
    "            null_values_df (pd.DataFrame): Summary of null values (output of `check_null_values()`).\n",
    "            drop_null_threshold (float): Threshold percentage for dropping columns.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        for _, row in null_values_df.iterrows():\n",
    "            dataset_name = row['Dataset']\n",
    "            column_name = row['Kolom']\n",
    "            null_percentage = row['Persentase Null Values (%)']\n",
    "            df = self.datasets.get(dataset_name)\n",
    "            if df is None:\n",
    "                self.logger.error(f\"Dataset '{dataset_name}' not found.\")\n",
    "                continue\n",
    "            if null_percentage > drop_null_threshold:\n",
    "                if column_name in df.columns:\n",
    "                    df.drop(column_name, axis=1, inplace=True)\n",
    "                    self.logger.info(f\"Column '{column_name}' in dataset '{dataset_name}' has been dropped because the null percentage ({null_percentage}%) exceeds the threshold.\")\n",
    "                else:\n",
    "                    self.logger.error(f\"Column '{column_name}' is not found in dataset '{dataset_name}' and cannot be dropped.\")\n",
    "                    continue\n",
    "            elif column_name in df.columns:\n",
    "                if pd.api.types.is_numeric_dtype(df[column_name]):\n",
    "                    skewness_value = df[column_name].dropna().skew()\n",
    "                    method = 'median' if skewness_value != 0 else 'mean'\n",
    "                    self.impute_value(df, column_name, method, dataset_name)\n",
    "                elif pd.api.types.is_datetime64_any_dtype(df[column_name]):\n",
    "                    self.impute_value(df, column_name, 'interpolate', dataset_name)\n",
    "                else:\n",
    "                    self.impute_value(df, column_name, 'mode', dataset_name)\n",
    "            else:\n",
    "                self.logger.error(f\"Column '{column_name}' does not exist in dataset '{dataset_name}', continue to next column.\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e2638662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Contoh data\n",
    "#     data = {\n",
    "#         'id': [1, 2, 3, 4, 5],\n",
    "#         'name': ['Alice', 'Bob', None, 'Dave', 'Eve'],\n",
    "#         'age': [25, 30, 35, None, 40],\n",
    "#         'signup_date': ['2022-01-01', '2022-02-01', '2022-03-01', '2022-04-01', '2022-05-01']\n",
    "#     }\n",
    "#     df = pd.DataFrame(data)\n",
    "#     datasets = {'example': df}\n",
    "\n",
    "#     dq = DataQuality(datasets)\n",
    "#     duplicates = dq.check_duplicates()\n",
    "#     nulls = dq.check_null_values()\n",
    "#     dq.handle_duplicates(duplicates)\n",
    "#     dq.handle_null_values(nulls)\n",
    "#     validation = dq.validate_data_types_and_ranges(\n",
    "#         expected_types={'id': 'int64', 'name': 'object', 'age': 'float64', 'signup_date': 'datetime64[ns]'},\n",
    "#         value_ranges={'age': (18, 100)}\n",
    "#     )\n",
    "#     print(\"Duplicates:\\n\", duplicates)\n",
    "#     print(\"\\nNull Values:\\n\", nulls)\n",
    "#     print(\"\\nValidation Results:\\n\", validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b20943",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 4: Extracting and Saving Data to Staging**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e562a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    A class for extracting data from CSV files and saving it into a staging area.\n",
    "\n",
    "    This class loads a configuration from a YAML file and provides methods to extract data from a CSV file\n",
    "    and save it in a Parquet format in the staging area.\n",
    "\n",
    "    Methods:\n",
    "        __init__(config_path: str) -> None:\n",
    "            Initializes the DataExtractor instance with a configuration path.\n",
    "\n",
    "        load_config() -> dict:\n",
    "            Loads the configuration from the YAML file.\n",
    "\n",
    "        extract_data(csv_file: str, staging_file: str) -> bool:\n",
    "            Extracts data from a CSV file and saves it in Parquet format to the staging area.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataExtractor instance.\n",
    "\n",
    "        Parameters:\n",
    "            config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.config_path = config_path\n",
    "        self.logger = Logger(__name__).get_logger()\n",
    "        self.db_connection = DatabaseConnection(config_path)\n",
    "\n",
    "    def extract_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts data from a CSV file and saves it in Parquet format to the staging area.\n",
    "\n",
    "        Parameters:\n",
    "            csv_file (str): The path to the CSV file to be extracted.\n",
    "            staging_file (str): The path where the extracted data should be saved in Parquet format.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns `True` if the data was successfully extracted and saved, `False` otherwise.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error during the extraction or saving process.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            conn, engine = self.db_connection.connect()\n",
    "            \n",
    "            query = \"\"\"\n",
    "                SELECT DISTINCT\n",
    "                    a.title, \n",
    "                    c.name AS category, \n",
    "                    d.date AS publication_date, \n",
    "                    a.url, \n",
    "                    a.description AS summary\n",
    "                FROM fact_news fn\n",
    "                JOIN dim_article a ON fn.news_id = a.news_id\n",
    "                JOIN dim_category c ON fn.category_id = c.category_id\n",
    "                JOIN dim_date d ON fn.date_id = d.date_id;\n",
    "            \"\"\"\n",
    "            data = pd.read_sql(query, engine)\n",
    "            self.logger.info(\"Data successfully extracted from data warehouse.\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to extract data: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5e2718a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs('./extracted', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "456cde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     config_path = './config.yaml'\n",
    "#     extractor = DataExtractor(config_path)\n",
    "#     data = extractor.extract_data()\n",
    "\n",
    "#     if data is not None:\n",
    "#         base_path = './'\n",
    "#         staging_file = f\"{base_path}extracted/staging_data.parquet\"\n",
    "#         data.to_parquet(staging_file, index=False)\n",
    "#         extractor.logger.info(f\"Data berhasil disimpan ke staging area ({staging_file}).\")\n",
    "#         display(data.head())\n",
    "#     else:\n",
    "#         extractor.logger.error(\"Tidak ada data yang diekstrak.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7f645c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dca299",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 5: Transforming and Validating Data**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7f8ee0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    A class for transforming and processing data loaded from a staging file.\n",
    "\n",
    "    This class provides methods to load configuration, read data from a staging area (parquet file),\n",
    "    and perform data transformation including data quality checks (duplicates, null values, data types, etc.).\n",
    "\n",
    "    Methods:\n",
    "        __init__(config_path: str) -> None:\n",
    "            Initializes the DataTransformer instance with a configuration path.\n",
    "            \n",
    "        load_config() -> dict:\n",
    "            Loads the configuration from the YAML file.\n",
    "        \n",
    "        process_data(staging_file: str) -> pd.DataFrame | None:\n",
    "            Reads data from the given staging file (parquet).\n",
    "        \n",
    "        transform_data(data: pd.DataFrame) -> pd.DataFrame | None:\n",
    "            Transforms the data by performing various quality checks and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataTransformer instance.\n",
    "\n",
    "        Parameters:\n",
    "            config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.config_path = config_path\n",
    "        self.logger = Logger(__name__).get_logger()\n",
    "        self.dq = None\n",
    "\n",
    "    def process_data(self, staging_file: str) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Reads data from the given staging file (parquet).\n",
    "\n",
    "        Parameters:\n",
    "            staging_file (str): The path to the parquet file containing the staging data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Returns the loaded data as a DataFrame, or None if loading fails.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error during the reading process.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            data = pd.read_parquet(staging_file)\n",
    "            self.logger.info(\"Data was successfully read from the staging area.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process data from staging area: {e}\")\n",
    "            return None\n",
    "\n",
    "    def transform_data(self, data: pd.DataFrame) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Transforms the data by performing various quality checks and validation.\n",
    "\n",
    "        This includes checking for duplicates, null values, and validating data types and ranges.\n",
    "\n",
    "        Parameters:\n",
    "            data (pd.DataFrame): The data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Returns the transformed data as a DataFrame, or None if transformation fails.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error during the transformation process.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.dq = DataQuality({'bbc_df': data})\n",
    "\n",
    "            duplicates_df = self.dq.check_duplicates()\n",
    "            self.dq.handle_duplicates(duplicates_df)\n",
    "\n",
    "            null_values_df = self.dq.check_null_values()\n",
    "            self.dq.handle_null_values(null_values_df, drop_null_threshold=25)\n",
    "\n",
    "            data['publication_date'] = pd.to_datetime(data['publication_date'], errors='coerce')\n",
    "\n",
    "            data['title'] = data['title'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "            data['summary'] = data['summary'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "            data['processed_date'] = pd.Timestamp.now()\n",
    "\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to transform data: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "53be6ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.makedirs('./transformed', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5c75abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     config_path = './config.yaml'\n",
    "#     transformer = DataTransformer(config_path)\n",
    "#     base_path = './'\n",
    "#     staging_file = f\"{base_path}extracted/staging_data.parquet\"\n",
    "#     data = transformer.process_data(staging_file)\n",
    "\n",
    "#     if data is not None:\n",
    "#         transformed_data = transformer.transform_data(data)\n",
    "\n",
    "#         if transformed_data is not None:\n",
    "#             # Simpan data yang telah di-transformasi\n",
    "#             transformed_file = f\"{base_path}transformed/transformed_data.parquet\"\n",
    "#             transformed_data.to_parquet(transformed_file, index=False)\n",
    "#             transformer.logger.info(f\"Data berhasil di-transformasi dan disimpan ke {transformed_file}.\")\n",
    "#             display(transformed_data.head())\n",
    "#         else:\n",
    "#             transformer.logger.error(\"Data tidak berhasil di-transformasi.\")\n",
    "#     else:\n",
    "#         transformer.logger.error(\"Data tidak berhasil diproses dari staging area.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c7019",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 6: Loading Clean Data to the Database**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "246ec1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    A class for loading data into dimension and fact tables in a database.\n",
    "\n",
    "    This class provides methods to load data into various dimension tables (e.g., `dim_date`, `dim_category`, \n",
    "    `dim_source_category`, `dim_tag`, and `dim_article`) as well as a fact table (`fact_news`). It also includes\n",
    "    a method for parsing date strings.\n",
    "\n",
    "    Methods:\n",
    "        __init__(config_path: str) -> None:\n",
    "            Initializes the DataLoader instance with a configuration file path.\n",
    "\n",
    "        load_to_dimension_tables(data: pd.DataFrame) -> bool:\n",
    "            Loads the provided data into various dimension tables in the database.\n",
    "\n",
    "        load_to_fact_table(data: pd.DataFrame) -> bool:\n",
    "            Loads the provided data into the fact table in the database.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DataLoader instance.\n",
    "\n",
    "        Parameters:\n",
    "            config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        self.config_path = config_path\n",
    "        self.logger = Logger(__name__).get_logger()\n",
    "        self.db_connection = DatabaseConnection(config_path)\n",
    "        \n",
    "    def process_data(self, staging_file: str) -> pd.DataFrame | None:\n",
    "        \"\"\"\n",
    "        Reads data from the given staging file (parquet).\n",
    "\n",
    "        Parameters:\n",
    "            staging_file (str): The path to the parquet file containing the staging data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame | None: Returns the loaded data as a DataFrame, or None if loading fails.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error during the reading process.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            data = pd.read_parquet(staging_file)\n",
    "            self.logger.info(\"Data was successfully read from the transformed area.\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to process data from transformed area: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_to_new_table(self, data: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Loads the provided data into a new table in the database.\n",
    "\n",
    "        This method creates a new table named `dim_cleaned` if it does not already exist,\n",
    "        and then loads the data into that table.\n",
    "        \n",
    "        Parameters:\n",
    "            data (pd.DataFrame): The data to be loaded into the database.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns `True` if the data was successfully loaded, `False` otherwise.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there is an error during the table creation or data loading process.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            conn, engine = self.db_connection.connect()\n",
    "            \n",
    "            create_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS dim_cleaned (\n",
    "                    cleaned_id SERIAL PRIMARY KEY,\n",
    "                    title VARCHAR(255) NOT NULL,\n",
    "                    category VARCHAR(100) NOT NULL,\n",
    "                    publication_date TIMESTAMP NOT NULL,\n",
    "                    url VARCHAR(255) NOT NULL,\n",
    "                    summary TEXT NOT NULL,\n",
    "                    processed_date TIMESTAMP NOT NULL\n",
    "                );\n",
    "            \"\"\"\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(create_table_query)\n",
    "                conn.commit()\n",
    "                self.logger.info(\"Table dim_cleaned was successfully created or already exists.\")\n",
    "\n",
    "            data.to_sql('dim_cleaned', engine, if_exists='append', index=False)\n",
    "            self.logger.info(\"Data successfully loaded into dim_cleaned table.\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create table dim_cleaned: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if self.db_connection:\n",
    "                self.db_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e31cae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     config_path = './config.yaml'\n",
    "#     loader = DataLoader(config_path)\n",
    "#     base_path = './'\n",
    "#     transformed_file = f\"{base_path}transformed/transformed_data.parquet\"\n",
    "#     data = loader.process_data(transformed_file)\n",
    "    \n",
    "#     if data is not None:\n",
    "#         succes = loader.load_to_new_table(data)\n",
    "\n",
    "#         if succes:\n",
    "#             loader.logger.info(\"Data berhasil dimuat ke tabel dim_cleaned.\")    \n",
    "#         else:\n",
    "#             loader.logger.error(\"Data tidak berhasil dimuat ke tabel dim_cleaned.\")\n",
    "#     else:\n",
    "#         loader.logger.error(\"Data tidak berhasil diproses dari staging area.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5236501d",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ **Step 7: Running the Full ETL Process**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    A class for managing the ETL (Extract, Transform, Load) pipeline.\n",
    "    \n",
    "    This class orchestrates the entire ETL process by coordinating the extraction, transformation,\n",
    "    and loading of data. It uses the `DataExtractor`, `DataTransformer`, and `DataLoader` classes\n",
    "    to perform each step of the pipeline. The pipeline is configured using a YAML file.\n",
    "\n",
    "    Methods:\n",
    "        __init__(config_path: str) -> None:\n",
    "            Initializes the ETL pipeline with a configuration path.\n",
    "            \n",
    "        _load_config() -> dict:\n",
    "            Loads configuration from the YAML file.\n",
    "\n",
    "        run() -> bool:\n",
    "            Executes the entire ETL pipeline, returning True if successful, False otherwise.        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the ETL pipeline with configuration path.\n",
    "\n",
    "        Parameters:\n",
    "            config_path (str): Path to the YAML configuration file.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        self.config_path = config_path\n",
    "        self.config = self._load_config()\n",
    "        self.extractor = DataExtractor(config_path)\n",
    "        self.transformer = DataTransformer(config_path)\n",
    "        self.loader = DataLoader(config_path)\n",
    "        self.logger = self.loader.logger \n",
    "\n",
    "    def _load_config(self) -> dict:\n",
    "        \"\"\"\n",
    "        Load the configuration from the YAML file.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Configuration settings loaded from the YAML file.\n",
    "        Raises:\n",
    "            Exception: If the configuration file cannot be read or parsed.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            with open(self.config_path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            self.loader.logger.error(f\"Failed to load configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    def run(self) -> bool:\n",
    "        \"\"\"\n",
    "        Execute the entire ETL pipeline.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if all steps succeeded, False otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # --- EXTRACT ---\n",
    "            extracted_data = self.extractor.extract_data()\n",
    "            if extracted_data is None:\n",
    "                self.logger.error(\"Extraction failed.\")\n",
    "                return False\n",
    "\n",
    "            staging_dir = self.config.get('staging_directory', './extracted')\n",
    "            os.makedirs(staging_dir, exist_ok=True)\n",
    "            staging_file = os.path.join(staging_dir, 'staging_data.parquet')\n",
    "            extracted_data.to_parquet(staging_file, index=False)\n",
    "            self.logger.info(f\"Data saved to staging: {staging_file}\")\n",
    "\n",
    "            # --- TRANSFORM ---\n",
    "            staging_data = self.transformer.process_data(staging_file)\n",
    "            if staging_data is None:\n",
    "                self.logger.error(\"Failed to load staging data.\")\n",
    "                return False\n",
    "\n",
    "            transformed_data = self.transformer.transform_data(staging_data)\n",
    "            if transformed_data is None:\n",
    "                self.logger.error(\"Transformation failed.\")\n",
    "                return False\n",
    "\n",
    "            transformed_dir = self.config.get('transformed_directory', './transformed')\n",
    "            os.makedirs(transformed_dir, exist_ok=True)\n",
    "            transformed_file = os.path.join(transformed_dir, 'transformed_data.parquet')\n",
    "            transformed_data.to_parquet(transformed_file, index=False)\n",
    "            self.logger.info(f\"Transformed data saved: {transformed_file}\")\n",
    "\n",
    "            # --- LOAD ---\n",
    "            loaded_data = self.loader.process_data(transformed_file)\n",
    "            if loaded_data is None:\n",
    "                self.logger.error(\"Failed to load transformed data.\")\n",
    "                return False\n",
    "\n",
    "            load_success = self.loader.load_to_new_table(loaded_data)\n",
    "            if not load_success:\n",
    "                self.logger.error(\"Database loading failed.\")\n",
    "                return False\n",
    "\n",
    "            self.logger.info(\"ETL pipeline completed successfully.\")\n",
    "            return True\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e26df3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_content = \"\"\"\n",
    "postgresql:\n",
    "  host: localhost\n",
    "  database: Assignment10-DWHModelling\n",
    "  user: postgres\n",
    "  password:\n",
    "\"\"\"\n",
    "with open('./config.yaml', 'w') as f:\n",
    "        f.write(yaml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6ab0979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 13:02:24,969 - __main__ - INFO - Database connection successfully opened.\n",
      "2025-05-13 13:02:25,015 - __main__ - INFO - Data successfully extracted from data warehouse.\n",
      "2025-05-13 13:02:25,016 - __main__ - INFO - Database connection closed.\n",
      "2025-05-13 13:02:25,022 - __main__ - INFO - Data saved to staging: ./extracted\\staging_data.parquet\n",
      "2025-05-13 13:02:25,033 - __main__ - INFO - Data was successfully read from the staging area.\n",
      "2025-05-13 13:02:25,036 - __main__ - INFO - Dataset 'bbc_df' has no duplicates.\n",
      "2025-05-13 13:02:25,039 - __main__ - INFO - Dataset 'bbc_df' has no null values.\n",
      "2025-05-13 13:02:25,048 - __main__ - INFO - Transformed data saved: ./transformed\\transformed_data.parquet\n",
      "2025-05-13 13:02:25,068 - __main__ - INFO - Data was successfully read from the transformed area.\n",
      "2025-05-13 13:02:25,093 - __main__ - INFO - Database connection successfully opened.\n",
      "2025-05-13 13:02:25,097 - __main__ - INFO - Table dim_cleaned was successfully created or already exists.\n",
      "2025-05-13 13:02:25,302 - __main__ - INFO - Data successfully loaded into dim_cleaned table.\n",
      "2025-05-13 13:02:25,303 - __main__ - INFO - Database connection closed.\n",
      "2025-05-13 13:02:25,303 - __main__ - INFO - ETL pipeline completed successfully.\n",
      "2025-05-13 13:02:25,304 - __main__ - INFO - All ETL steps completed successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_path = './config.yaml'\n",
    "    pipeline = DataPipeline(config_path)\n",
    "    \n",
    "    if pipeline.run():\n",
    "        pipeline.logger.info(\"All ETL steps completed successfully.\")\n",
    "    else:\n",
    "        pipeline.logger.error(\"ETL pipeline encountered errors.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "day11_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
