# ğŸ“¦ DAY 11 - ASSIGNMENT

## ğŸš€ Project Overview

This project implements an ETL (Extract, Transform, Load) pipeline to retrieve news data from a data warehouse, clean and structure it for analysis, and load the processed data back into a dedicated table. The process ensures high-quality, well-formatted data suitable for downstream analytics or reporting.

> ğŸ›¢ï¸ **Database used**: PostgreSQL

> ğŸ§© **Connected via**: DBeaver (a powerful database management tool for developers and analysts)

## ğŸ“ Project Structure

The project is organized as follows:

```
.
â”œâ”€â”€ extracted/                      # ğŸ” Raw data extraction outputs
â”‚   â””â”€â”€ staging_data.parquet        # ğŸ“¦ Staged data file
â”œâ”€â”€ logs/                           # ğŸ“ Pipeline logs
â”‚   â””â”€â”€ etl_pipeline.log            # ğŸ“– Execution log file
â”œâ”€â”€ transformed/                    # ğŸ”„ Transformed data outputs
â”‚   â””â”€â”€ transformed_data.parquet    # ğŸ’¾ Cleaned and processed data
â”œâ”€â”€ config.yaml                     # ğŸ—ƒ Pipeline and database configuration
â”œâ”€â”€ etl.ipynb                       # ğŸ“Š Jupyter notebook for interactive ETL exploration
â”œâ”€â”€ README.md                       # ğŸ“˜ Project documentation (this file)
â””â”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
```

## âœ¨ Features

* ğŸ“¤ **Extract**: Read raw data from sources and write to Parquet (`extracted/staging_data.parquet`).
* ğŸ§¹ **Transform**: Clean, normalize, and enrich the data; output saved to `transformed/transformed_data.parquet`.
* ğŸ§¾ **Logging**: Record pipeline steps and errors in `logs/etl_pipeline.log`.
* ğŸ““ **Interactive Notebook**: `etl.ipynb` for data exploration and ad hoc transformations.

## âœ… Prerequisites

* ğŸ Python 3.10 or higher
* ğŸ˜ PostgreSQL database
* ğŸ–¥ï¸ DBeaver for managing and exploring the database
* ğŸ“¦ Required Python libraries (see `requirements.txt`)

## âš™ï¸ Installation

1. Clone the repository:

   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

2. Set up a virtual environment:

   ```bash
   python -m venv venv
   venv\Scripts\activate  # On Windows
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. **Configure settings**:

   * Edit `config.yaml` to adjust file paths, database credentials (if used), and other parameters.

## ğŸ› ï¸ Usage

> ğŸ¯ **Usage:** Run the entire pipeline interactively through Jupyter Notebook.

1. Launch the notebook:

   ```bash
   jupyter notebook etl.ipynb
   ```
2. Execute all cells (Run All) to perform extraction, transformation, and review the data end-to-end.

## ğŸ”„ ETL Pipeline Workflow

1. **Extract** ğŸ“¤: Retrieve required news fields (title, category, date, URL, summary) from the Data Warehouse on [Day 10 - Assignment](https://github.com/firdh0/Bootcamp_Dibimbing_Data-Engineering/tree/main/Day%2010_Data%20Warehouse%20Modeling/Assignment)
2. **Transform** ğŸ§¹: Cleanse, standardize, and enrich the data (remove duplicates, format dates, trim text, add processing timestamp).
3. **Load** ğŸ“¦: Save the processed dataset back into a separate table in the Data Warehouse, ensuring no duplicates or data loss.

## ğŸ”§ Configuration

The database connection and other configurations are stored in `config/config.yml`. Update this file with your database credentials and other settings.

Example `config.yml`:

```yaml
database:
  host: "localhost"
  database: "your_database"
  user: "your_username"
  password: "your_password"
```

## ğŸ—’ï¸ Logging

Logs are saved in `logs/etl_pipeline.log`. The log file contains detailed information about the pipeline's execution, including errors and warnings.

## ğŸ“„ Dependencies

The required Python libraries are listed in `requirements.txt`. Install them using:

```bash
pip install -r requirements.txt
```

## ğŸ“Š Example Output

### âœ… 1. Data Loaded to PostgreSQL (via DBeaver)

ğŸ–¥ï¸ *Visualized in DBeaver*

**Table: `dim_cleaned`**
![dim_category Table](./image/Screenshot%202025-05-13%20133956.png)
![dim_category Table](./image/Screenshot%202025-05-13%20134317.png)
